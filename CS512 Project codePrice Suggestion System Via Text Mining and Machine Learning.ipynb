{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "#from bokeh.transform import factor_cmap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  go through each of variables with a brief statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t')\n",
    "test = pd.read_csv('test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of training and dataset\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different data types in the dataset: categorical (strings) and numeric\n",
    "train.dtypes\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price\n",
    "train.price.describe()\n",
    "plt.subplot(1, 2, 1)\n",
    "(train['price']).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,250])\n",
    "plt.xlabel('price+', fontsize=17)\n",
    "plt.ylabel('frequency', fontsize=17)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.title('Price Distribution - Training Set', fontsize=17)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "np.log(train['price']+1).plot.hist(bins=50, figsize=(20,10), edgecolor='white')\n",
    "plt.xlabel('log(price+1)', fontsize=17)\n",
    "plt.ylabel('frequency', fontsize=17)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.title('Log(Price) Distribution - Training Set', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shipping\n",
    "train.shipping.value_counts()/len(train)\n",
    "prc_shipBySeller = train.loc[train.shipping==1, 'price']\n",
    "prc_shipByBuyer = train.loc[train.shipping==0, 'price']\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.hist(np.log(prc_shipBySeller+1), color='#8CB4E1', alpha=1.0, bins=50,\n",
    "       label='Price when Seller pays Shipping')\n",
    "ax.hist(np.log(prc_shipByBuyer+1), color='#007D00', alpha=0.7, bins=50,\n",
    "       label='Price when Buyer pays Shipping')\n",
    "ax.set(title='Histogram Comparison', ylabel='% of Dataset in Bin')\n",
    "plt.xlabel('log(price+1)', fontsize=17)\n",
    "plt.ylabel('frequency', fontsize=17)\n",
    "plt.title('Price Distribution by Shipping Type', fontsize=17)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item Category\n",
    "print(\"There are %d unique values in the category column.\" % train['category_name'].nunique())\n",
    "# TOP 5 RAW CATEGORIES\n",
    "train['category_name'].value_counts()[:5]\n",
    "# missing categories\n",
    "print(\"There are %d items that do not have a label.\" % train['category_name'].isnull().sum())\n",
    "# reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "    #Execution sequence is: *train['category_name'], apply, zip()\n",
    "#####\\ change line\n",
    "train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\n",
    "zip(*train['category_name'].apply(lambda x: split_cat(x)))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same step for the test set\n",
    "test['general_cat'], test['subcat_1'], test['subcat_2'] = \\\n",
    "zip(*test['category_name'].apply(lambda x: split_cat(x)))\n",
    "print(\"There are %d unique general main-categories.\" % train['general_cat'].nunique())\n",
    "print(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())\n",
    "print(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())\n",
    "x = train['general_cat'].value_counts().index.values.astype('str')\n",
    "y = train['general_cat'].value_counts().values\n",
    "pct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))]  #text showed havoring over the spline\n",
    "trace1 = go.Bar(x=x, y=y, text=pct)\n",
    "layout = dict(title= 'Number of Items by Main Category',\n",
    "              yaxis = dict(title='Count'),\n",
    "              xaxis = dict(title='Category'))\n",
    "fig=dict(data=[trace1], layout=layout)\n",
    "py.iplot(fig)\n",
    "x = train['subcat_1'].value_counts().index.values.astype('str')[:15]\n",
    "y = train['subcat_1'].value_counts().values[:15]\n",
    "pct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))][:15]\n",
    "trace1 = go.Bar(x=x, y=y, text=pct,\n",
    "                marker=dict(\n",
    "                color = y,colorscale='Portland',showscale=True,\n",
    "                reversescale = False\n",
    "                ))\n",
    "layout = dict(title= 'Number of Items by Sub Category (Top 15)',\n",
    "              yaxis = dict(title='Count'),\n",
    "              xaxis = dict(title='SubCategory'))\n",
    "fig=dict(data=[trace1], layout=layout)\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brand name\n",
    "print(\"There are %d unique brand names in the training dataset.\" % train['brand_name'].nunique())\n",
    "x = train['brand_name'].value_counts().index.values.astype('str')[:10]\n",
    "y = train['brand_name'].value_counts().values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item Description\n",
    "def wordCount(text):\n",
    "    # convert to lower case and strip regex\n",
    "    try:\n",
    "         # convert to lower case and strip regex\n",
    "        text = text.lower()\n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        txt = regex.sub(\" \", text)\n",
    "        # tokenize\n",
    "        # words = nltk.word_tokenize(clean_txt)\n",
    "        # remove words in stop words\n",
    "        words = [w for w in txt.split(\" \") \\\n",
    "                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n",
    "        return len(words)\n",
    "    except: \n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of word counts to both the training and test set\n",
    "train['desc_len'] = train['item_description'].apply(lambda x: wordCount(x))\n",
    "test['desc_len'] = test['item_description'].apply(lambda x: wordCount(x))\n",
    "\n",
    "train.head()\n",
    "df = train.groupby('desc_len')['price'].mean().reset_index()\n",
    "df.head()\n",
    "trace1 = go.Scatter(\n",
    "    x = df['desc_len'],\n",
    "    y = np.log(df['price']+1),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'lines+markers'\n",
    ")\n",
    "layout = dict(title= 'Average Log(Price) by Description Length',\n",
    "              yaxis = dict(title='Average Log(Price)'),\n",
    "              xaxis = dict(title='Description Length'))\n",
    "fig=dict(data=[trace1], layout=layout)\n",
    "py.iplot(fig)\n",
    "train.item_description.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values in item description\n",
    "train = train[pd.notnull(train['item_description'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of words for each category\n",
    "cat_desc = dict()\n",
    "for cat in general_cats: \n",
    "    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n",
    "    cat_desc[cat] =  tokenize(text)\n",
    "\n",
    "# flat list of all words combined\n",
    "flat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\n",
    "allWordsCount = Counter(flat_lst)\n",
    "all_top10 = allWordsCount.most_common(20)\n",
    "x = [w[0] for w in all_top10]\n",
    "y = [w[1] for w in all_top10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(x=x, y=y, text=pct)\n",
    "layout = dict(title= 'Word Frequency',\n",
    "              yaxis = dict(title='Count'),\n",
    "              xaxis = dict(title='Word'))\n",
    "fig=dict(data=[trace1], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing: tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        #sent_tokenize , word_tokenize are method in ntlk\n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map reduce filter  http://book.pythontips.com/en/latest/map_filter.html \n",
    "# apply the tokenizer into the item descriptipn column\n",
    "# https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas\n",
    "#apply works on a row / column basis of a DataFrame, applymap works element-wise on a DataFrame, and map works element-wise on a Series.\n",
    "train['tokens'] = train['item_description'].map(tokenize)\n",
    "test['tokens'] = test['item_description'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for description, tokens in zip(train['item_description'].head(),\n",
    "                              train['tokens'].head()):\n",
    "    print('description:', description)\n",
    "    print('tokens:', tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary with key=category and values as all the descriptions related.\n",
    "cat_desc = dict()\n",
    "for cat in general_cats: \n",
    "    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n",
    "    cat_desc[cat] = tokenize(text)\n",
    "\n",
    "\n",
    "# find the most common words for the top 4 categories\n",
    "women100 = Counter(cat_desc['Women']).most_common(100)\n",
    "beauty100 = Counter(cat_desc['Beauty']).most_common(100)\n",
    "kids100 = Counter(cat_desc['Kids']).most_common(100)\n",
    "electronics100 = Counter(cat_desc['Electronics']).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(tup):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                          max_words=50, max_font_size=40,\n",
    "                          random_state=42\n",
    "                         ).generate(str(tup))\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 2, figsize=(30, 15))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(generate_wordcloud(women100), interpolation=\"bilinear\")\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Women Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(generate_wordcloud(beauty100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Beauty Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(generate_wordcloud(kids100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Kids Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(generate_wordcloud(electronics100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Electronic Top 100\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing: tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             max_features=180000,\n",
    "                             tokenizer=tokenize,\n",
    "                             ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc = np.append(train['item_description'].values, test['item_description'].values)\n",
    "vz = vectorizer.fit_transform(list(all_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a dictionary mapping the tokens to their tfidf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n",
    "                    dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by=['tfidf'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by=['tfidf'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "trn = train.copy()\n",
    "tst = test.copy()\n",
    "trn['is_train'] = 1\n",
    "tst['is_train'] = 0\n",
    "\n",
    "sample_sz = 15000\n",
    "\n",
    "combined_df = pd.concat([trn, tst])\n",
    "combined_sample = combined_df.sample(n=sample_sz)\n",
    "vz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_comp=30\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "svd_tfidf = svd.fit_transform(vz_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600,\n",
    "                       title=\"tf-idf clustering of the item description\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sample.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\n",
    "tfidf_df['description'] = combined_sample['item_description']\n",
    "tfidf_df['tokens'] = combined_sample['tokens']\n",
    "tfidf_df['category'] = combined_sample['general_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 30 # need to be selected wisely\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n",
    "                               init='k-means++',\n",
    "                               n_init=1,\n",
    "                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    aux = ''\n",
    "    for j in sorted_centroids[i, :10]:\n",
    "        aux += terms[j] + ' | '\n",
    "    print(aux)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same steps for the sample\n",
    "kmeans = kmeans_model.fit(vz_sample)\n",
    "kmeans_clusters = kmeans.predict(vz_sample)\n",
    "kmeans_distances = kmeans.transform(vz_sample)\n",
    "# reduce dimension to 2 using tsne\n",
    "tsne_kmeans = tsne_model.fit_transform(kmeans_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_sample.reset_index(drop=True, inplace=True)\n",
    "kmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\n",
    "kmeans_df['cluster'] = kmeans_clusters\n",
    "kmeans_df['description'] = combined_sample['item_description']\n",
    "kmeans_df['category'] = combined_sample['general_cat']\n",
    "#kmeans_df['cluster']=kmeans_df.cluster.astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans = bp.figure(plot_width=700, plot_height=600,\n",
    "                        title=\"KMeans clustering of the description\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n",
    "                                    color=colormap[kmeans_clusters],\n",
    "                                    description=kmeans_df['description'],\n",
    "                                    category=kmeans_df['category'],\n",
    "                                    cluster=kmeans_df['cluster']))\n",
    "\n",
    "plot_kmeans.scatter(x='x', y='y', color='color', source=source)\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\n",
    "show(plot_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent Dirichlet Allocation\n",
    "cvectorizer = CountVectorizer(min_df=4,\n",
    "                              max_features=180000,\n",
    "                              tokenizer=tokenize,\n",
    "                              ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvz = cvectorizer.fit_transform(combined_sample['item_description'])\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                      learning_method='online',\n",
    "                                      max_iter=20,\n",
    "                                      random_state=42)\n",
    "X_topics = lda_model.fit_transform(cvz)\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.components_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension to 2 using tsne\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)\n",
    "unnormalized = np.matrix(X_topics)\n",
    "doc_topic = unnormalized/unnormalized.sum(axis=1)\n",
    "\n",
    "lda_keys = []\n",
    "for i, tweet in enumerate(combined_sample['item_description']):\n",
    "    lda_keys += [doc_topic[i].argmax()]\n",
    "\n",
    "lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\n",
    "lda_df['description'] = combined_sample['item_description']\n",
    "lda_df['category'] = combined_sample['general_cat']\n",
    "lda_df['topic'] = lda_keys\n",
    "lda_df['topic'] = lda_df['topic'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lda = bp.figure(plot_width=700,\n",
    "                     plot_height=600,\n",
    "                     title=\"LDA topic visualization\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n",
    "                                    color=colormap[lda_keys],\n",
    "                                    description=lda_df['description'],\n",
    "                                    topic=lda_df['topic'],\n",
    "                                    category=lda_df['category']))\n",
    "\n",
    "plot_lda.scatter(source=source, x='x', y='y', color='color')\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\":\"@description\",\n",
    "                \"topic\":\"@topic\", \"category\":\"@category\"}\n",
    "show(plot_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareLDAData():\n",
    "    data = {\n",
    "        'vocab': vocab,\n",
    "        'doc_topic_dists': doc_topic,\n",
    "        'doc_lengths': list(lda_df['len_docs']),\n",
    "        'term_frequency':cvectorizer.vocabulary_,\n",
    "        'topic_term_dists': lda_model.components_\n",
    "    } \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "lda_df['len_docs'] = combined_sample['tokens'].map(len)\n",
    "ldadata = prepareLDAData()\n",
    "pyLDAvis.enable_notebook()\n",
    "prepared_data = pyLDAvis.prepare(**ldadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define RMSL Error Function\n",
    "def rmsle(Y, Y_pred):\n",
    "    # Y and Y_red have already been in log scale.\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    return np.sqrt(np.mean(np.square(Y_pred - Y )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_table('../input/train.tsv')\n",
    "test_df = pd.read_table('../input/test.tsv')\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing data.\n",
    "def fill_missing_values(df):\n",
    "    df.category_name.fillna(value=\"Other\", inplace=True)\n",
    "    df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.fillna(value=\"None\", inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = fill_missing_values(train_df)\n",
    "test_df = fill_missing_values(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale target variable to log.\n",
    "train_df[\"target\"] = np.log1p(train_df.price)\n",
    "\n",
    "# Split training examples into train/dev examples.\n",
    "train_df, dev_df = train_test_split(train_df, random_state=347, train_size=0.99)\n",
    "\n",
    "Y_train = train_df.target.values.reshape(-11, 1)\n",
    "Y_dev = dev_df.target.values.reshape(-1, 1)\n",
    "\n",
    "# Calculate number of train/dev/test examples.\n",
    "n_trains = train_df.shape[0]\n",
    "n_devs = dev_df.shape[0]\n",
    "n_tests = test_df.shape[0]\n",
    "print(\"Training on\", n_trains, \"examples\")\n",
    "print(\"Validating on\", n_devs, \"examples\")\n",
    "print(\"Testing on\", n_tests, \"examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN Model\n",
    "# Concatenate train - dev - test data for easy to handle\n",
    "full_df = pd.concat([train_df, dev_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Processing categorical data...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(full_df.category_name)\n",
    "full_df.category_name = le.transform(full_df.category_name)\n",
    "\n",
    "le.fit(full_df.brand_name)\n",
    "full_df.brand_name = le.transform(full_df.brand_name)\n",
    "\n",
    "del le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Transforming text data to sequences...\")\n",
    "raw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower()])\n",
    "\n",
    "print(\"   Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "print(\"   Transforming text to sequences...\")\n",
    "full_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\n",
    "full_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n",
    "\n",
    "del tok_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants to use when define RNN model\n",
    "MAX_NAME_SEQ = 10\n",
    "MAX_ITEM_DESC_SEQ = 75\n",
    "MAX_TEXT = np.max([\n",
    "    np.max(full_df.seq_name.max()),\n",
    "    np.max(full_df.seq_item_description.max()),\n",
    "]) + 4\n",
    "MAX_CATEGORY = np.max(full_df.category_name.max()) + 1\n",
    "MAX_BRAND = np.max(full_df.brand_name.max()) + 1\n",
    "MAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_keras_data(df):\n",
    "    X = {\n",
    "        'name': pad_sequences(df.seq_name, maxlen=MAX_NAME_SEQ),\n",
    "        'item_desc': pad_sequences(df.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n",
    "        'brand_name': np.array(df.brand_name),\n",
    "        'category_name': np.array(df.category_name),\n",
    "        'item_condition': np.array(df.item_condition_id),\n",
    "        'num_vars': np.array(df[[\"shipping\"]]),\n",
    "    }\n",
    "    return X\n",
    "\n",
    "train = full_df[:n_trains]\n",
    "dev = full_df[n_trains:n_trains+n_devs]\n",
    "test = full_df[n_trains+n_devs:]\n",
    "\n",
    "X_train = get_keras_data(train)\n",
    "X_dev = get_keras_data(dev)\n",
    "X_test = get_keras_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define RNN model\n",
    "def new_rnn_model(lr=0.001, decay=0.0):    \n",
    "    # Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    category_name = Input(shape=[1], name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_name = Embedding(MAX_TEXT, 20)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n",
    "\n",
    "    # rnn layers\n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_name)\n",
    "\n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name),\n",
    "        Flatten() (emb_category_name),\n",
    "        item_condition,\n",
    "        rnn_layer1,\n",
    "        rnn_layer2,\n",
    "        num_vars,\n",
    "    ])\n",
    "\n",
    "    main_l = Dense(256)(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    main_l = Dense(128)(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    main_l = Dense(64)(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    # the output layer.\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "\n",
    "    model = Model([name, item_desc, brand_name , category_name, item_condition, num_vars], output)\n",
    "\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = new_rnn_model()\n",
    "model.summary()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set hyper parameters for the model.\n",
    "BATCH_SIZE = 1024\n",
    "epochs = 2\n",
    "\n",
    "# Calculate learning rate decay.\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(n_trains / BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.007, 0.0005\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "\n",
    "print(\"Fitting RNN model to training examples...\")\n",
    "rnn_model.fit(\n",
    "        X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_dev, Y_dev), verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Evaluating the model on validation data...\")\n",
    "Y_dev_preds_rnn = rnn_model.predict(X_dev, batch_size=BATCH_SIZE)\n",
    "print(\" RMSLE error:\", rmsle(Y_dev, Y_dev_preds_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_preds = rnn_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "rnn_preds = np.expm1(rnn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train - dev - test data for easy to handle\n",
    "full_df = pd.concat([train_df, dev_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert data type to string\n",
    "full_df['shipping'] = full_df['shipping'].astype(str)\n",
    "full_df['item_condition_id'] = full_df['item_condition_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Vectorizing data...\")\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "def build_preprocessor(field):\n",
    "    field_idx = list(full_df.columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=50000,\n",
    "        preprocessor=build_preprocessor('name'))),\n",
    "    ('category_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('category_name'))),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('brand_name'))),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('shipping'))),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('item_condition_id'))),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=100000,\n",
    "        preprocessor=build_preprocessor('item_description'))),\n",
    "])\n",
    "\n",
    "X = vectorizer.fit_transform(full_df.values)\n",
    "\n",
    "X_train = X[:n_trains]\n",
    "X_dev = X[n_trains:n_trains+n_devs]\n",
    "X_test = X[n_trains+n_devs:]\n",
    "\n",
    "print(X.shape, X_train.shape, X_dev.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Fitting Ridge model on training examples...\")\n",
    "ridge_model = Ridge(\n",
    "    solver='auto', fit_intercept=True, alpha=0.5,\n",
    "    max_iter=100, normalize=False, tol=0.05,\n",
    ")\n",
    "ridge_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "from IPython.core.display import display, HTML, Javascript\n",
    "\n",
    "#h = IPython.display.display(HTML(html_string))\n",
    "#IPython.display.display_HTML(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
